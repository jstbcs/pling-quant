---
title: "Individual Differences of Quantifier Interpretation"
author: "Julia Haaf"
date: "6/3/2019"
bibliography      : "lab.bib"
output: 
  html_document:
    theme: cerulean
    highlight: tango
    code_folding: hide
---

```{r, message=F}
library("dplyr")
library("MCMCpack")
library("LaplacesDemon")
library("rstan")
library("plotrix")
library(RColorBrewer)
```

**To Do:**

- Extend to other quantifiers (many < more than half < most vs. more than half < most < many).
- Look at symmetry models for fewer than half vs. more than half, and few vs. many.
- Repeat analysis for study 2 data.

## The Task (How I understand it)

In this task, participants have to evaluate whether a certain quantifier logically describes a scenario. Scenarios are percentages of, for example, apples that are green. Say, 20% of apples are green, is the statement that "most of the apples are green" true or false? Percentages are drawn from a uniform ensuring that as many trials have more than 50% as have less than 50%. The quantifiers of interest are *Few*, *Fewer than half*, *Many*, *Most* and *More than half*. The idea is that *More than half* is defined by more than 50% for everyone while *Most* may be more subjective, and it may be more than *More than half* for everyone. Additionally, we may inspect the relationship between *Many* and *More than half*, and also symmetries between mirrored quantifiers.

## Modeling Approach {.tabset .tabset-fade}

I think we could use a probit regression with a participant-specific quantifier effect and a participant-specific effect of percentage used. This way we could at least assess whether *Most* is more than *More than half* by defining the quantifier effect as a contrast between the two. It could even be tested in this framework whether *More than half* is 50% for everyone.

### Model

Let $i$ indicate participants, $i = 1, \ldots, I$, $j$ indicate the quantifier, $j = 1, \ldots, 5$, and $k$ indicate the trial for each quantifier, $k = 1, \ldots, K_{ij}$.^[Originally, $K_{ij} = 50$. But this value may be reduced after cleaning on the trial level.] Then $Y_{ijk}$ is the $i$th participant's response to the $j$th quantifier in the $k$th trial, and $Y_{ijk} = 1$ if participants indicate *true*, and $Y_{ijk} = 0$ if participant indicate *false*. Then, we may model $Y_{ijk}$ as a Bernoulli, using the probit link function on the probabilities:

\begin{align*}
Y_{ijk} &\sim \mbox{Bernoulli}(\pi_{ijk}),\\
\mu_{ijk} &= \Phi^{-1}(\pi_{ijk}),\\
\end{align*}

where the second line maps the probability space of $\pi$ onto the real space of $\mu$. We may now place a linear model on $\mu_{ijk}$:

\[\mu_{ijk} = \beta_{i0} + l_{ijk}\beta_{i1} + h_{ijk}\beta_{i2} + z_{i1} \beta_{i3} +  + z_{i2} \beta_{i4} +  + z_{i3} \beta_{i5} +  + z_{i4} \beta_{i6} + z_{i5} \beta_{i7},\]

where $l_{ijk}$, $h_{ijk}$ and $z_{ij}$ are predictors: $l_{ijk}$ is zero for percentages above 50%, and indicates the centered percentage otherwise; $xh{ijk}$ is zero for percentages below 50%, and indicates the centered percentage otherwise, and $z_{ij}$ is an indicator function denoting the respective quantifier (e.g., $z_{i1} = 1$ for the quantifier *Fewer*, and $z_{i1} = 0$ otherwise). Parameters $\beta_{i0}$ are random intercepts, $\beta_{i1}$ are random percentage effects for percentages below 50%, $\beta_{i12}$ are random percentage effects for percentages above 50%, and $\beta_{i3}$ to $\beta_{i7}$ are random quantifier effects. For now, I will place the following priors:

\begin{align*}
\beta_{i0} &\sim \mbox{Normal}(0, \sigma_0^2),\\
\beta_{i1} &\sim \mbox{Normal}(\delta_1, \sigma_1^2),\\
\beta_{i2} &\sim \mbox{Normal}(\delta_2, \sigma_2^2),\\
\beta_{i3} &\sim \mbox{Normal}(\delta_3, \sigma_3^2).\\
\beta_{i4} &\sim \mbox{Normal}(\delta_4, \sigma_4^2).\\
\beta_{i5} &\sim \mbox{Normal}(\delta_5, \sigma_5^2).\\
\beta_{i6} &\sim \mbox{Normal}(\delta_6, \sigma_6^2).\\
\beta_{i7} &\sim \mbox{Normal}(\delta_7, \sigma_7^2).\\
\end{align*}

### Simulating From the Model

```{r}
set.seed(123)
I <- 90
J <- 2
K <- 50
sub <- rep(1:I, each = J*K)
quant <- factor(rep(1:J, each = K, times = I), labels = c("More", "Most"))
offperc <- c(quant[26:(I*J*K)], quant[1:25])
perc <- ifelse(offperc == 1, sample(1:50, replace = T), sample(50:100, replace = T))
hist(perc)

z <- as.numeric(quant) - 1
x <- (perc - 50)/100
l <- (2 - offperc) * x
h <- (offperc - 1) * x

s0 <- 1
s1 <- .05
s2 <- .1
s3 <- .1
d1 <- 15
d2 <- 10
d3 <- -.5

b0 <- rnorm(I, 0, s0)
b1 <- rnorm(I, d1, s1)
b2 <- rnorm(I, d2, s2)
b3 <- rnorm(I, d3, s3)

mu <- b0 + l * b1 + h * b2 + z * b3
p <- pnorm(mu)
hist(p)
# p <- invlogit(mu)
# hist(p)
y <- rbinom(I*J*K, 1, p)
table(y, quant)
```

```{r}
prop <- tapply(y, list(perc, quant), mean)
matplot(as.numeric(rownames(prop)), prop
        , pch = 19, col = c("firebrick", "slateblue")
        , xlab = "Percent")
abline(v = 50, lwd = 1.5, col = "darkgrey")
```

```{r thresholds}
threshmore <- -b0/b2 * 100 + 50
threshmost <- (-b0 - b3)/b2 * 100 + 50

plot(threshmore, threshmost
     , pch = 19, col = "gray40"
     , ylab = "Threshold for 'most of'"
     , xlab = "Threshold for 'more than'")
abline(0, 1, lwd = 1.5, col = "firebrick")
```

### Data

```{r}
exclude <- c(0, 1, 5, 18, 32, 39, 49, 52, 53, 58, 62, 81, 82, 83, 85, 86, 87, 27, 76)
exclude2 <- c(27, 76)

dat <- read.csv("data/exp1-replication-trials.csv")
head(dat)
nrow(dat)

# Exclusion based on Sonia's code
## Participant-level
dat <- subset(dat, !(workerid %in% exclude))
nrow(dat)

## Trial-level
dat <- subset(dat, read_and_decide_time > 300)
dat <- subset(dat, read_and_decide_time < 2500)
nrow(dat)

table(dat$quant, dat$workerid)[, 1]
hist(dat$percent)
table(dat$percent>50)

table(dat$response, dat$quant)
dat$qq <- as.numeric(factor(dat$quant))

dat <- subset(dat, qq %in% 2:6)
prop <- tapply(as.numeric(factor(dat$response)) - 1, list(dat$percent, dat$qq), mean, na.rm = T)
```

```{r fig-quant}
qcols <- brewer.pal(5, "Dark2")

matplot(as.numeric(rownames(prop)), prop
        , pch = 19, col = qcols
        , xlab = "Percent", ylab = "Proportion 'true' responses"
        , frame.plot = F)
abline(v = 50, lwd = 1.5, col = "darkgrey")
abline(h = .50, lwd = 1.5, col = "darkgrey")
legend(75, .7, legend = c("Few", "Fewer than half", "Many", "More than half", "Most")
       , fill = qcols, bg = "white", box.col = "white")

```

```{r, eval = F}
datsub <- subset(dat, workerid == 27)
prop <- tapply(as.numeric(datsub$response) - 1, list(datsub$percent, datsub$quant), mean)
matplot(as.numeric(rownames(prop)), prop
        , pch = 19, col = c(adjustcolor("firebrick", .5), adjustcolor("slateblue", .5))
        , xlab = "Percent")
abline(v = 50, lwd = 1.5, col = "darkgrey")
```

```{r, eval = F}
dat <- read.csv("data/exp1-replication-v2-trials.csv")
head(dat)
table(dat$quant, dat$workerid)[, 1]
hist(dat$percent)
table(dat$percent>50)

table(dat$response, dat$quant)
dat$qq <- as.numeric(dat$quant) %in% c(5, 6)

dat <- subset(dat, qq == T)
prop <- tapply(as.numeric(dat$response) - 1, list(dat$percent, dat$quant), mean)
matplot(as.numeric(rownames(prop)), prop
        , pch = 19, col = c("firebrick", "slateblue")
        , xlab = "Percent")
abline(v = 50, lwd = 1.5, col = "darkgrey")
```

### Estimation

#### Priors

```{r}
x <- seq(0, 5, .01)
ysig <- dlnorm(x, .1, .5)
ysig2 <- 2 * x * dinvgamma(x^2, 2, .1)
plot(x, ysig, type = "l")
lines(x, ysig2, col = 2)
ydelt <- rnorm(x, 0, .1)

M <- 10000
ysig2 <- rinvgamma(M, 2, .1)
ydelt <- rnorm(M, 0, .1)

prioreff <- rnorm(M, ydelt, sqrt(ysig2))

layout(matrix(1:4, ncol = 2, byrow = T))
hist(prioreff)
hist(ydelt)
hist(pnorm(prioreff), xlim = c(0, 1))
hist(pnorm(ydelt), xlim = c(0, 1))
```

#### Model

```{r logmod, cache = T}
logmod <- "

data {
  int<lower=1> D;                     // #Dimensions of the model
  int<lower=0> N;                     // #Observations
  int<lower=1> I;                     // #Participants
  int<lower=0,upper=1> y[N];          // Data 0,1
  vector[N] cperc;                    // Centered Percentages
  int<lower=1,upper=I> sub[N];        // participant vector
  int<lower=0,upper=1> few[N];      // Few
  int<lower=0,upper=1> fewer[N];      // Fewer than half
  int<lower=0,upper=1> many[N];      // Many
  int<lower=0,upper=1> more[N];      // More than half
  int<lower=0,upper=1> most[N];      // Most
  int<lower=0,upper=1> above[N];      // Above 50 percent?
}

parameters {
  real delta[D];                      // Means of betas
  real<lower=0> sigma2[D];             // variance of betas
  vector[D] beta[I];                  // vectors of betas
}

transformed parameters {
  real<lower=0> sigma[D];
  sigma = sqrt(sigma2);
}

model {
  delta ~ normal(0, 5);
  sigma2 ~ inv_gamma(2, .1);
  for (i in 1:I)
    beta[i] ~ normal(delta, sigma);
  for (n in 1:N)
    y[n] ~ bernoulli(Phi_approx(beta[sub[n], 1] + 
    (1 - above[n]) * cperc[n] * beta[sub[n], 2] + 
    above[n] * cperc[n] * beta[sub[n], 3] + 
    few[n] * beta[sub[n], 4] + 
    fewer[n] * beta[sub[n], 5] + 
    many[n] * beta[sub[n], 6] + 
    more[n] * beta[sub[n], 7] + 
    most[n] * beta[sub[n], 8]));
}
"

logmodM <- stan_model(model_code = logmod)
```

```{r init, cache = T}
getDat <- function(dat){
  D <- 8
  N <- nrow(dat)
  I <- length(unique(dat$workerid))
  
  y <- as.numeric(factor(dat$response)) - 1
  cperc <- (dat$percent - 50) / 100
  sub <- as.numeric(factor(dat$workerid))
  few <- ifelse(dat$qq == 2, 1, 0)
  fewer <- ifelse(dat$qq == 3, 1, 0)
  many <- ifelse(dat$qq == 4, 1, 0)
  more <- ifelse(dat$qq == 5, 1, 0)
  most <- ifelse(dat$qq == 6, 1, 0)
  above <- as.numeric(cperc > 0)
  
  standat <- list(D = D, N = N, I = I, y = y, cperc = cperc, sub = sub,
                  few = few, fewer = fewer, many = many, more = more, most = most
                  , above = above)  
  return(standat)
}

myRunner <- function(standat, iter = 1000, warmup = 400, mod, nchains = 4, ...){
  inits <- list(delta = rep(0, standat$D), sigma = rep(1, standat$D), beta = matrix(0, nrow = standat$I, ncol = standat$D))
  # inits <- list(c1 = inits)
  fit <- sampling(mod, verbose=T,
                  data = standat, 
                  iter = iter,
                  warmup = warmup,
                  chains = nchains,
                  cores = nchains,
                  # init = inits,
                  pars = c("beta", "delta", "sigma")
                  , ...)
  return(fit)}
```

```{r run-logmod, cache = T}
rerun <- T
predat <- getDat(dat)
logmodfit <- myRunner(predat
                      , iter = 2800
                      , warmup = 800
                      , mod = logmodM
                      , control = list(adapt_delta = .95)
                      , nchains = 4)
save(logmodfit, file = "outstudy1.rda")
```

```{r}
hist(summary(logmodfit)$summary[,"Rhat"]
     , breaks = 100
     , main = ""
     , xlab =  "Rhat")

hist(summary(logmodfit)$summary[,"n_eff"]
     , breaks = 100
     , main = ""
     , xlab =  "Number of effective samples")
```

```{r inspect-logmod, fig.asp=1.6}
pEst <- as.matrix(logmodfit)
dim(pEst)

pMeans <- colMeans(pEst) 

I <- predat$I
pBeta0 <- pMeans[1:I]
pBeta1 <- pMeans[(I + 1):(2 * I)]
pBeta2 <- pMeans[(2*I + 1):(3 * I)]
pBeta3 <- pMeans[(3*I + 1):(4 * I)]
pBeta4 <- pMeans[(4*I + 1):(5 * I)]
pBeta5 <- pMeans[(5*I + 1):(6 * I)]
pBeta6 <- pMeans[(6*I + 1):(7 * I)]
pBeta7 <- pMeans[(7*I + 1):(8 * I)]

below <- c((1:50 - 50)/100, rep(0, 50))
mbelow <- matrix(rep(below, I), nrow = I, byrow = T)
above <- c(rep(0, 50), (51:100 - 50)/100)
mabove <- matrix(rep(above, I), nrow = I, byrow = T)
sided <- c(rep(0, 50), rep(1, 50))
msided <- matrix(rep(sided, I), nrow = I, byrow = T)

layout(matrix(1:3, ncol = 1))

pMu1 <- matrix(rep(pBeta0, 100), ncol = 100) + mbelow * pBeta1 + mabove * pBeta2 + matrix(rep(pBeta3, 100), ncol = 100)
pps1 <- pnorm(pMu1)

pMu2 <- matrix(rep(pBeta0, 100), ncol = 100) + mbelow * pBeta1 + mabove * pBeta2 + matrix(rep(pBeta4, 100), ncol = 100)
pps2 <- pnorm(pMu2)

pMu3 <- matrix(rep(pBeta0, 100), ncol = 100) + mbelow * pBeta1 + mabove * pBeta2 + matrix(rep(pBeta5, 100), ncol = 100)
pps3 <- pnorm(pMu3)

pMu4 <- matrix(rep(pBeta0, 100), ncol = 100) + mbelow * pBeta1 + mabove * pBeta2 + matrix(rep(pBeta6, 100), ncol = 100)
pps4 <- pnorm(pMu4)

pMu5 <- matrix(rep(pBeta0, 100), ncol = 100) + mbelow * pBeta1 + mabove * pBeta2 + matrix(rep(pBeta7, 100), ncol = 100)
pps5 <- pnorm(pMu5)

plot(1:100, pnorm(colMeans(pps1)), type = "l", lwd = 2, col = qcols[1], ylim = c(0,1))
lines(1:100, pnorm(colMeans(pps2)), lwd = 2, col = qcols[2])
lines(1:100, pnorm(colMeans(pps3)), lwd = 2, col = qcols[3])
lines(1:100, pnorm(colMeans(pps4)), lwd = 2, col = qcols[4])
lines(1:100, pnorm(colMeans(pps5)), lwd = 2, col = qcols[5])
abline(h = .5, col = adjustcolor(1, .5))
abline(v = 50, col = adjustcolor(1, .5))

layout(matrix(c(0, 1,1,2,2,0, 3,3,4,4,5,5), nrow = 2, byrow = T))

matplot(1:100, t(pps1), type = "l", lty = 1, main = "Few", ylim = c(0,1))
abline(h = .5, col = adjustcolor(1, .5))
abline(v = 50, col = adjustcolor(1, .5))

matplot(1:100, t(pps2), type = "l", lty = 1, main = "Fewer than half", ylim = c(0,1))
abline(h = .5, col = adjustcolor(1, .5))
abline(v = 50, col = adjustcolor(1, .5))

matplot(1:100, t(pps3), type = "l", lty = 1, main = "Many", ylim = c(0,1))
abline(h = .5, col = adjustcolor(1, .5))
abline(v = 50, col = adjustcolor(1, .5))

matplot(1:100, t(pps4), type = "l", lty = 1, main = "More than half", ylim = c(0,1))
abline(h = .5, col = adjustcolor(1, .5))
abline(v = 50, col = adjustcolor(1, .5))

matplot(1:100, t(pps5), type = "l", lty = 1, main = "Most", ylim = c(0,1))
abline(h = .5, col = adjustcolor(1, .5))
abline(v = 50, col = adjustcolor(1, .5))
```

```{r thresholds-exp1, fig.asp=1.5}
threshmore <- ifelse((-pBeta0/pBeta2 * 100 + 50) > 50
                     , -pBeta0/pBeta2 * 100 + 50
                     , -pBeta0/pBeta1 * 100 + 50)
threshmost <- ifelse(((-pBeta0 - pBeta3)/pBeta2 * 100 + 50) > 50
                     , (-pBeta0 - pBeta3)/pBeta2 * 100 + 50
                     , (-pBeta0 - pBeta3)/pBeta1 * 100 + 50)
  

layout(matrix(c(1,2,3,3), ncol = 2, byrow = T), heights = c(.4, .6))

plot(threshmore, pch = 19, col = adjustcolor(1, .5), ylim = c(0, 100))
abline(h = 50, lwd = 1.5, col = "firebrick")
plot(threshmost, pch = 19, col = adjustcolor(1, .5), ylim = c(0, 100))
abline(h = 50, lwd = 1.5, col = "firebrick")

plot(threshmore, threshmost
     , pch = 19, col = adjustcolor(1, .5)
     , ylab = "Threshold for 'most of'"
     , xlab = "Threshold for 'more than'"
     , xlim = c(0, 100)
     , ylim = c(0, 100))
abline(0, 1, lwd = 1.5, col = "firebrick")
```

```{r CrIs}
pCIs <- apply(pEst, 2, quantile, probs = c(.025, .975)) 

ciBeta3 <- pCIs[, (3*I + 1):(4 * I)]
```

```{r}
indord <- order(pBeta3)
plotCI(1:I, pBeta3[indord]
       , ui = ciBeta3[2, indord], li = ciBeta3[1, indord]
       , pch = 21, pt.bg = "slateblue"
       , ylab = "More than - Most of"
       , xlab = "Participant")
abline(h = 0)
```

```{r}
## Prior probability
M <- 100000
s2 <- rinvgamma(M, 2, .1)
beta3 <- rnorm(M, 0, .1)

prioreff <- exp(pnorm(0, beta3, sqrt(s2), lower.tail = T, log.p = T) * I)
priorneg <- mean(prioreff)

## Posterior Probability
targ <- pEst[, (3*I + 1):(4 * I)]
good <- targ < 0 #evaluate their sign
all.good <- apply(good, 1, mean) #evaluate how often all theta estimates are postitive
postneg <- mean(all.good == 1) #Posterior probability of all theta_i being positive

priorneg
postneg

postneg/priorneg
```

